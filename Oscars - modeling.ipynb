{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data collected from Metacritic, IMDB, and Oscars.org\n",
    "file = 'df_master_1992-2017_cleaned'\n",
    "f = open(file,'rb') \n",
    "df = pickle.load(f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop_duplicates(['imdb_name', 'imdb_year'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bp = df[df.bp_nominee == 1.0]\n",
    "len(df_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noms = df[df.nominee == 1.0]\n",
    "len(df_noms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.winner.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sets of comparisons:  \n",
    "* Best Picture nominees vs. other\n",
    "* Best Picture winners vs. Best Picture nominees that didn't win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('bp_nominee')['log_gross_new_norm', 'metacritic_norm', 'runtime', 'log_total_wins_norm'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bp.groupby('winner')['log_gross_new_norm', 'metacritic_norm', 'runtime', 'log_total_wins_norm'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('bp_nominee')['comedy', 'drama', 'mystery', 'family', 'sci-fi', 'musical',\n",
    "                        'thriller', 'history', 'adventure', 'sport', 'biography', 'USA', 'UK', 'Winter'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bp.groupby('winner')['comedy', 'drama', 'mystery', 'family', 'sci-fi', 'musical',\n",
    "                        'thriller', 'history', 'adventure', 'sport', 'biography', 'USA', 'UK',  'Spring'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_genre = pd.crosstab(df.Winter, df.bp_nominee)\n",
    "winner_genre = pd.crosstab(df_bp.Winter, df_bp.winner)\n",
    "\n",
    "ax = bp_genre.div(bp_genre.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "ax = winner_genre.div(winner_genre.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Best Picture by Season')\n",
    "plt.xlabel('Winter')\n",
    "plt.ylabel('Percentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a general classifer function\n",
    "def train_score(classifier, x, y, test_size):\n",
    "    mm = MinMaxScaler()\n",
    "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=test_size, random_state=1234)\n",
    "    xtrain = mm.fit_transform(xtrain)\n",
    "    xtest = mm.transform(xtest)\n",
    "    ytrain = np.ravel(ytrain)    \n",
    "    clf = classifier.fit(xtrain, ytrain)    \n",
    "    \n",
    "    # score the model (accuracy)   \n",
    "    train_acc = clf.score(xtrain, ytrain)\n",
    "    test_acc = clf.score(xtest, ytest)\n",
    "    \n",
    "    print(\"Training Data Accuracy: %0.2f\" %(train_acc))\n",
    "    print(\"Test Data Accuracy:     %0.2f\" %(test_acc))\n",
    "    \n",
    "    # create a confusion matrix\n",
    "    y_true = ytest\n",
    "    y_pred = clf.predict(xtest)   \n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    print ('\\n')\n",
    "    print(conf)\n",
    "\n",
    "    print ('\\n')\n",
    "    print (\"Precision:              %0.2f\" %(conf[1, 1] / (conf[1, 1] + conf[0, 1])))\n",
    "    print (\"Recall:                 %0.2f\"% (conf[1, 1] / (conf[1, 1] + conf[1, 0])))\n",
    "    \n",
    "    # ROC curve\n",
    "    y_score = clf.predict_proba(xtest)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(ytest, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print('AUC: ', roc_auc)    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([0,1],[0,1]) # this is our baseline\n",
    "    plt.plot(fpr, tpr) # this is our ROC curve\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['log_gross_new_norm', 'drama', 'comedy', 'action', 'crime', 'romance', 'sport', 'biography',\n",
    "              'mystery', 'musical', 'thriller', 'adventure', 'sci-fi', 'family', 'history', 'USA', 'UK',\n",
    "              'runtime', 'metacritic_norm', 'log_total_wins_norm', 'Winter', 'Spring']\n",
    "y = df['bp_nominee']\n",
    "X = df.loc[:, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in features:\n",
    "    print(df.groupby('bp_nominee')[var].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the goal is to predict the Best Picture *winner*, the models are designed to reduce bias by predicting Best Picture *nominees*. The highest predicted probability of nomination will then be used as a proxy for the 'winner'.\n",
    "\n",
    "8 different classifiers are used:  \n",
    "* Logistic regression\n",
    "* Naive Bayes\n",
    "* KNN\n",
    "* SVM\n",
    "* Random forest\n",
    "* Extra Trees\n",
    "* Gradient boosting\n",
    "* AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "\n",
    "rfe_lr = RFE(model_lr, 20)\n",
    "train_score(rfe_lr, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_lr.ranking_), features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe_Bayes = RFE(model_Bayes, 20)\n",
    "train_score(rfe_Bayes, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_Bayes.ranking_), features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "xtrain = mm.fit_transform(xtrain)\n",
    "xtest = mm.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "\n",
    "for k in range(1, 20):\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, xtrain, ytrain, cv=5, scoring='recall')\n",
    "    \n",
    "    k_scores.append((k, scores.mean()))\n",
    "    \n",
    "k_scores = pd.DataFrame(k_scores, columns=['k', 'recall']).set_index('k')\n",
    "k_scores[k_scores.recall==max(k_scores.recall)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=1)\n",
    "model_knn.fit(xtrain, ytrain)\n",
    "\n",
    "y_pred = model_knn.predict(xtest)\n",
    "y_score = model_knn.predict_proba(xtest)[:,1]\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(accuracy_score(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "xtrain_sc = mm.fit_transform(xtrain)\n",
    "xtest_sc = mm.transform(xtest)\n",
    "\n",
    "param_grid = {'C': [1.0, 10., 100.],\n",
    "              'degree':[2,3,4],\n",
    "              'kernel':['rbf'],\n",
    "              'gamma':[3,4]}\n",
    "\n",
    "svm = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "svm.fit(xtrain_sc, ytrain)\n",
    "y_pred = svm.predict(xtest_sc)\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(accuracy_score(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 20, max_depth = 3, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.2)\n",
    "print (model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 20, max_depth = 3, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 20, max_depth = 3, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.2)\n",
    "print (model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 20, max_depth = 3, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "\n",
    "model = ExtraTreesClassifier(max_features = 20, max_depth = 3, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model2 = RFE(lr, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "model2.fit(X, y)\n",
    "model3.fit(X, y)\n",
    "\n",
    "#df['y_pred'] = model2.predict(X)\n",
    "df['y_pred_prob1'] = model.predict_proba(X)[:,1]\n",
    "df['y_pred_prob2'] = model2.predict_proba(X)[:,1]\n",
    "\n",
    "df['y_pred_prob_ensem'] = (df['y_pred_prob1'] + df['y_pred_prob2'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the max probability by year to flag the 'winner'\n",
    "pred_winners = df.loc[df.groupby('oscar_year')['y_pred_prob_ensem'].idxmax(),['imdb_name']]\n",
    "pred_winners['pred_winner'] = 1.0\n",
    "results = pd.merge(df, pred_winners, on='imdb_name', how='outer')\n",
    "results['pred_winner'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(results['winner'], results['pred_winner']))\n",
    "print(accuracy_score(results['winner'], results['pred_winner']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine which movies were classified correctly and which were classified incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Best Picture winners were categorized correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[(results['winner'] == 1.0) & (results['pred_winner'] == 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Best Picture winners were categorized incorrectly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[(results['winner'] == 1.0) & (results['pred_winner'] == 0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which were the \"snubs\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[(results['winner'] == 0.0) & (results['pred_winner'] ==1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the biggest snub ever?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = results.loc[(results['winner'] == 0.0) & (results['pred_winner'] == 1.0)].y_pred_prob_ensem.idxmax()\n",
    "\n",
    "print(\"The biggest Best Picture snub in the last 25 years:\")\n",
    "print('\\n')\n",
    "print('\\t' + results['imdb_name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export the final results\n",
    "results.to_csv('final results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
